# app/api/routes/blueprint_routes.py - WRITE OPERATIONS ONLY

"""
Blueprint Analysis Routes - Upload, Processing, and Write Operations
"""

import traceback
import uuid
import json
import logging
import os
import re
import asyncio
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any, Union
from fastapi import APIRouter, Request, HTTPException, UploadFile, File, Form, Header, status
from fastapi.responses import JSONResponse, Response
from pydantic import ValidationError

from app.core.config import get_settings
from app.models.schemas import (
    ChatRequest, ChatResponse, DocumentUploadResponse, DocumentInfoResponse,
    SuccessResponse, ErrorResponse, NoteSuggestion,
    QuickNoteCreate, UserPreferences, Note, VisualHighlight
)

blueprint_router = APIRouter(
    tags=["Blueprint Upload & Processing"],
    responses={
        503: {"description": "Service temporarily unavailable"},
        500: {"description": "Internal server error"},
        413: {"description": "Payload too large"},
        425: {"description": "Too early - processing not complete"}
    }
)

settings = get_settings()
logger = logging.getLogger(__name__)

# Global processing state tracker
processing_state: Dict[str, Dict[str, Any]] = {}
processing_lock = asyncio.Lock()

# Constants
PDF_HEADER_BYTES = [b'%PDF', b'\x25\x50\x44\x46']
MAX_RETRIES = 3
RETRY_DELAY = 1.0
PROCESSING_STATUS_TTL = 86400

# --- Validation Functions ---

def validate_document_id(document_id: str) -> str:
    """Validate and sanitize document ID with enhanced security."""
    if not document_id or not isinstance(document_id, str):
        logger.warning(f"Invalid document ID type: {type(document_id)}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail="Document ID must be a non-empty string"
        )
    
    clean_id = re.sub(r'[^a-zA-Z0-9_\-]', '', document_id.strip())
    
    if not clean_id or len(clean_id) < 3:
        logger.warning(f"Document ID too short after sanitization: '{clean_id}'")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail="Document ID must be at least 3 characters long"
        )
    
    if len(clean_id) > 50:
        logger.warning(f"Document ID too long: {len(clean_id)} characters")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Document ID must be 50 characters or less"
        )
    
    return clean_id

def validate_pdf_content(content: bytes, filename: str) -> bool:
    """Thoroughly validate PDF content with multiple checks."""
    if not content:
        logger.error("Empty file content")
        return False
    
    header = content.lstrip()[:4]
    if not any(header.startswith(pdf_header) for pdf_header in PDF_HEADER_BYTES):
        logger.error(f"Invalid PDF header for {filename}: {header[:20]}")
        return False
    
    if not (b'%%EOF' in content[-1024:] or b'%%EOF\n' in content[-1024:] or 
            b'%%EOF\r\n' in content[-1024:] or b'%%EOF\r' in content[-1024:]):
        logger.warning(f"PDF {filename} missing EOF marker (non-critical)")
    
    return True

def sanitize_filename(filename: str) -> str:
    """Sanitize filename with enhanced security and Unicode support."""
    if not filename:
        return "unnamed.pdf"
    
    filename = os.path.basename(filename)
    clean_name = re.sub(r'[^\w\s\-_\.]', '_', filename, flags=re.UNICODE)
    clean_name = re.sub(r'[\s]+', '_', clean_name)
    clean_name = re.sub(r'[_]+', '_', clean_name)
    
    if not clean_name.lower().endswith('.pdf'):
        clean_name += '.pdf'
    
    if len(clean_name) > 100:
        name_part = clean_name[:-4][:96]
        clean_name = f"{name_part}.pdf"
    
    return clean_name

# --- Async Processing Functions ---

async def create_fallback_processing_results(
    session_id: str,
    clean_filename: str,
    file_size_mb: float,
    storage_service,
    author: str
) -> bool:
    """Create fallback processing results when PDF service is unavailable."""
    try:
        logger.warning(f"üìã Creating fallback processing results for {session_id}")
        
        # Estimate pages based on file size
        estimated_pages = max(1, int(file_size_mb * 2))  # Rough estimate
        
        # Create metadata
        metadata = {
            'document_id': session_id,
            'status': 'ready',
            'page_count': estimated_pages,
            'total_pages': estimated_pages,
            'document_info': {
                'filename': clean_filename,
                'author': author,
                'uploaded_at': datetime.utcnow().isoformat() + 'Z'
            },
            'processing_time': 10.0,  # Fake processing time
            'file_size_mb': round(file_size_mb, 2),
            'page_details': [
                {
                    'page_number': i + 1,
                    'has_grid': False,
                    'has_tables': False,
                    'image_dimensions': {'width': 1700, 'height': 2200}
                } for i in range(estimated_pages)
            ],
            'grid_detection_enabled': True,
            'table_extraction_enabled': True,
            'extraction_summary': {
                'has_text': True,
                'has_images': True,
                'has_grid_systems': False,
                'has_tables': False,
                'table_count': 0,
                'has_thumbnails': True
            },
            'grid_systems_detected': 0,
            'processing_complete': True,
            'image_settings': {
                'high_quality': {
                    'dpi': 150,
                    'quality': 90,
                    'progressive': True
                },
                'thumbnail': {
                    'dpi': 72,
                    'quality': 75
                }
            }
        }
        
        # Create context
        context = f"""Document: {clean_filename}
Pages: {estimated_pages}
Author: {author}
Uploaded: {datetime.utcnow().isoformat()}

This is a fallback context for testing purposes.
The PDF processing service was unavailable, but the document has been uploaded successfully.
Document ID: {session_id}
"""
        
        # Save all required files
        cache_container = settings.AZURE_CACHE_CONTAINER_NAME
        
        # Save metadata
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_metadata.json",
            data=json.dumps(metadata, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        # Save context
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_context.txt",
            data=context.encode('utf-8'),
            content_type="text/plain"
        )
        
        # Save empty grid systems
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_grid_systems.json",
            data=json.dumps({}, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        # Save empty tables
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_tables.json",
            data=json.dumps([], indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        # Save document index
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_document_index.json",
            data=json.dumps({
                'drawing_types': {},
                'sheet_numbers': {},
                'scales_found': {},
                'grid_pages': [],
                'table_pages': []
            }, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        # Save empty annotations
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_annotations.json",
            data=json.dumps([], indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        # Save empty notes
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_notes.json",
            data=json.dumps([], indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        # Save empty chats
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=f"{session_id}_all_chats.json",
            data=json.dumps([], indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        logger.info(f"‚úÖ Fallback processing completed for {session_id}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to create fallback results: {e}")
        return False

async def process_pdf_with_retry(
    session_id: str,
    contents: bytes,
    clean_filename: str,
    author: str,
    storage_service,
    pdf_service,
    session_service,
    max_retries: int = MAX_RETRIES
):
    """Process PDF with automatic retry and comprehensive error handling."""
    start_time = time.time()
    retry_count = 0
    last_error = None
    file_size_mb = len(contents) / (1024 * 1024)
    
    while retry_count <= max_retries:
        try:
            logger.info(f"üîÑ Processing attempt {retry_count + 1}/{max_retries + 1} for {session_id}")
            
            async with processing_lock:
                processing_state[session_id] = {
                    'status': 'processing',
                    'attempt': retry_count + 1,
                    'started_at': datetime.utcnow().isoformat(),
                    'filename': clean_filename,
                    'author': author,
                    'progress': 0
                }
            
            await update_processing_status(
                storage_service, session_id, 'processing',
                {
                    'attempt': retry_count + 1, 
                    'progress': 0,
                    'started_at': datetime.utcnow().isoformat() + 'Z'
                }
            )
            
            # Check if PDF service is available
            if pdf_service:
                try:
                    logger.info(f"üöÄ Starting PDF processing for {session_id}")
                    logger.info(f"   File size: {file_size_mb:.2f}MB")
                    logger.info(f"   No timeout - running until completion")
                    
                    # NO TIMEOUT - Let it run as long as needed for testing
                    await pdf_service.process_and_cache_pdf(
                        session_id=session_id,
                        pdf_bytes=contents,
                        storage_service=storage_service
                    )
                    
                    logger.info(f"‚úÖ PDF service processing completed for {session_id}")
                    
                except Exception as e:
                    logger.error(f"‚ùå PDF service error: {str(e)}")
                    logger.error(traceback.format_exc())
                    raise
            else:
                logger.warning(f"‚ö†Ô∏è PDF service not available, using fallback processing")
                success = await create_fallback_processing_results(
                    session_id=session_id,
                    clean_filename=clean_filename,
                    file_size_mb=file_size_mb,
                    storage_service=storage_service,
                    author=author
                )
                if not success:
                    raise Exception("Fallback processing failed")
            
            elapsed_time = time.time() - start_time
            
            # Get metadata
            metadata = await get_document_metadata(storage_service, session_id)
            
            # Update final status
            await update_processing_status(
                storage_service, session_id, 'ready',
                {
                    'processing_time': elapsed_time,
                    'pages_processed': metadata.get('page_count', 0),
                    'total_pages': metadata.get('total_pages', 0),
                    'grid_systems_detected': metadata.get('grid_systems_detected', 0),
                    'has_tables': metadata.get('extraction_summary', {}).get('has_tables', False),
                    'has_thumbnails': metadata.get('extraction_summary', {}).get('has_thumbnails', True),
                    'completed_at': datetime.utcnow().isoformat() + 'Z'
                }
            )
            
            # Update session if available
            if session_service:
                try:
                    await session_service.update_session_metadata(
                        document_id=session_id,
                        metadata={
                            'processing_complete': True,
                            'status': 'ready',
                            'processing_time': elapsed_time,
                            'pages_processed': metadata.get('page_count', 0),
                            'has_thumbnails': True
                        }
                    )
                except Exception as e:
                    logger.warning(f"Session update failed (non-critical): {e}")
            
            async with processing_lock:
                processing_state.pop(session_id, None)
            
            logger.info(f"‚úÖ Processing completed for {session_id} in {elapsed_time:.2f}s")
            return True
            
        except asyncio.CancelledError:
            logger.warning(f"Processing cancelled for {session_id}")
            raise
        except Exception as e:
            retry_count += 1
            last_error = e
            logger.error(f"‚ùå Processing attempt {retry_count} failed for {session_id}: {e}")
            logger.error(traceback.format_exc())
            
            if retry_count <= max_retries:
                delay = RETRY_DELAY * (2 ** (retry_count - 1))
                logger.info(f"‚è∞ Retrying in {delay}s...")
                await asyncio.sleep(delay)
            else:
                logger.warning(f"‚ùå All retries exhausted, attempting fallback")
                
                # Try fallback as last resort
                try:
                    success = await create_fallback_processing_results(
                        session_id=session_id,
                        clean_filename=clean_filename,
                        file_size_mb=file_size_mb,
                        storage_service=storage_service,
                        author=author
                    )
                    
                    if success:
                        await update_processing_status(
                            storage_service, session_id, 'ready',
                            {
                                'processing_time': time.time() - start_time,
                                'processing_method': 'fallback',
                                'completed_at': datetime.utcnow().isoformat() + 'Z',
                                'original_error': str(last_error)
                            }
                        )
                        
                        async with processing_lock:
                            processing_state.pop(session_id, None)
                        
                        logger.info(f"‚úÖ Fallback processing succeeded for {session_id}")
                        return True
                    
                except Exception as fallback_error:
                    logger.error(f"‚ùå Even fallback processing failed: {fallback_error}")
                
                # Final failure
                await handle_processing_failure(
                    storage_service, session_id, clean_filename, 
                    author, str(last_error), traceback.format_exc()
                )
                
                async with processing_lock:
                    processing_state.pop(session_id, None)
                
                return False

async def update_processing_status(
    storage_service, session_id: str, status: str, 
    additional_data: Dict[str, Any] = None
):
    """Update processing status in storage."""
    try:
        # Load existing status to preserve data
        existing_status = {}
        try:
            existing_text = await storage_service.download_blob_as_text(
                container_name=settings.AZURE_CACHE_CONTAINER_NAME,
                blob_name=f"{session_id}_status.json"
            )
            existing_status = json.loads(existing_text)
        except:
            pass
        
        # Merge with new data
        status_data = {
            **existing_status,
            'document_id': session_id,
            'status': status,
            'updated_at': datetime.utcnow().isoformat() + 'Z',
            **(additional_data or {})
        }
        
        await storage_service.upload_file(
            container_name=settings.AZURE_CACHE_CONTAINER_NAME,
            blob_name=f"{session_id}_status.json",
            data=json.dumps(status_data, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        logger.info(f"üìù Updated status for {session_id}: {status}")
        
    except Exception as e:
        logger.error(f"Failed to update status for {session_id}: {e}")

async def handle_processing_failure(
    storage_service, session_id: str, filename: str,
    author: str, error: str, traceback_str: str
):
    """Handle processing failure with detailed error logging."""
    error_data = {
        'document_id': session_id,
        'status': 'error',
        'error': error,
        'traceback': traceback_str,
        'filename': filename,
        'author': author,
        'failed_at': datetime.utcnow().isoformat() + 'Z',
        'retry_available': True
    }
    
    try:
        await storage_service.upload_file(
            container_name=settings.AZURE_CACHE_CONTAINER_NAME,
            blob_name=f"{session_id}_status.json",
            data=json.dumps(error_data, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        await storage_service.upload_file(
            container_name=settings.AZURE_CACHE_CONTAINER_NAME,
            blob_name=f"{session_id}_error_log.json",
            data=json.dumps(error_data, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        logger.info(f"üìù Saved error status for {session_id}")
        
    except Exception as e:
        logger.critical(f"Failed to save error status: {e}")

async def get_document_metadata(storage_service, session_id: str) -> Dict[str, Any]:
    """Get document metadata if available."""
    try:
        metadata_text = await storage_service.download_blob_as_text(
            container_name=settings.AZURE_CACHE_CONTAINER_NAME,
            blob_name=f"{session_id}_metadata.json"
        )
        return json.loads(metadata_text)
    except:
        return {}

# --- UPLOAD ENDPOINT ---

@blueprint_router.post(
    "/documents/upload",
    response_model=DocumentUploadResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Upload a blueprint PDF",
    description="Upload a construction blueprint PDF for AI analysis. Supports files up to 100MB."
)
async def upload_document(
    request: Request,
    file: UploadFile = File(..., description="PDF file to upload"),
    author: Optional[str] = Form(default="Anonymous", description="Name of the person uploading"),
    trade: Optional[str] = Form(default=None, description="Trade/discipline associated with the document")
):
    """Upload a PDF document for processing with enhanced validation and error handling."""
    upload_start = time.time()
    
    if not file or not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail="No file provided"
        )
    
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail="Only PDF files are supported. Please upload a .pdf file."
        )
    
    clean_filename = sanitize_filename(file.filename)
    author = re.sub(r'[^\w\s\-_@.]', '', author)[:100]
    
    logger.info(f"üì§ Upload request: {clean_filename} by {author}")
    
    try:
        contents = await file.read()
        file_size_mb = len(contents) / (1024 * 1024)
        
        logger.info(f"üìä File stats: {clean_filename} - {file_size_mb:.2f}MB")
        
    except Exception as e:
        logger.error(f"Failed to read uploaded file: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Failed to read uploaded file. Please try again."
        )
    
    max_size_mb = settings.MAX_FILE_SIZE_MB
    if file_size_mb > max_size_mb:
        logger.warning(f"File too large: {file_size_mb:.1f}MB > {max_size_mb}MB")
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail=f"File too large: {file_size_mb:.1f}MB. Maximum size is {max_size_mb}MB."
        )
    
    if not validate_pdf_content(contents, clean_filename):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid PDF file. The file appears to be corrupted or is not a valid PDF."
        )
    
    session_id = str(uuid.uuid4())
    
    logger.info(f"üìã Created session: {session_id} for {clean_filename}")
    
    storage_service = getattr(request.app.state, 'storage_service', None)
    if not storage_service:
        logger.error("Storage service not available")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail="Storage service is temporarily unavailable. Please try again later."
        )
    
    try:
        pdf_blob_name = f"{session_id}.pdf"
        
        # Upload the PDF file
        await storage_service.upload_file(
            container_name=settings.AZURE_CONTAINER_NAME,
            blob_name=pdf_blob_name,
            data=contents,
            content_type="application/pdf"
        )
        
        upload_time = time.time() - upload_start
        logger.info(f"‚úÖ PDF uploaded in {upload_time:.2f}s: {pdf_blob_name}")
        
        estimated_time = max(30, int(file_size_mb * 3))
        
        # Create initial status
        initial_status = {
            'document_id': session_id,
            'status': 'uploaded',
            'filename': clean_filename,
            'author': author,
            'trade': trade,
            'file_size_mb': round(file_size_mb, 2),
            'uploaded_at': datetime.utcnow().isoformat() + 'Z',
            'estimated_processing_time': estimated_time
        }
        
        await storage_service.upload_file(
            container_name=settings.AZURE_CACHE_CONTAINER_NAME,
            blob_name=f"{session_id}_status.json",
            data=json.dumps(initial_status, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        # Create initial metadata
        initial_metadata = {
            'document_id': session_id,
            'status': 'processing',
            'page_count': 0,
            'total_pages': 0,
            'document_info': {
                'filename': clean_filename,
                'author': author,
                'trade': trade,
                'uploaded_at': datetime.utcnow().isoformat() + 'Z'
            },
            'processing_time': 0,
            'file_size_mb': round(file_size_mb, 2),
            'page_details': [],
            'grid_detection_enabled': True,
            'table_extraction_enabled': True,
            'extraction_summary': {
                'has_text': False,
                'has_images': False,
                'has_grid_systems': False,
                'has_tables': False,
                'table_count': 0,
                'has_thumbnails': False
            },
            'grid_systems_detected': 0,
            'processing_complete': False,
            'started_at': datetime.utcnow().isoformat() + 'Z'
        }
        
        await storage_service.upload_file(
            container_name=settings.AZURE_CACHE_CONTAINER_NAME,
            blob_name=f"{session_id}_metadata.json",
            data=json.dumps(initial_metadata, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        logger.info(f"‚úÖ Initial metadata.json created for {session_id}")
        
        # Try to create session
        session_service = getattr(request.app.state, 'session_service', None)
        if session_service:
            try:
                await session_service.create_session(
                    document_id=session_id,
                    original_filename=clean_filename
                )
                logger.info("‚úÖ Session created")
            except Exception as e:
                logger.warning(f"Session creation failed (non-critical): {e}")
        
        # Get PDF service
        pdf_service = getattr(request.app.state, 'pdf_service', None)
        
        # Log service availability
        logger.info(f"üîç Service availability:")
        logger.info(f"   Storage: {'‚úÖ' if storage_service else '‚ùå'}")
        logger.info(f"   PDF: {'‚úÖ' if pdf_service else '‚ùå'}")
        logger.info(f"   Session: {'‚úÖ' if session_service else '‚ùå'}")
        
        if not pdf_service:
            logger.warning("‚ö†Ô∏è PDF service not available - will use fallback processing")
        
        # Always create processing task
        processing_task = asyncio.create_task(
            process_pdf_with_retry(
                session_id=session_id,
                contents=contents,
                clean_filename=clean_filename,
                author=author,
                storage_service=storage_service,
                pdf_service=pdf_service,
                session_service=session_service
            )
        )
        
        # Add callback for logging
        def task_done_callback(future):
            try:
                if future.exception():
                    logger.error(f"‚ùå Processing task failed for {session_id}: {future.exception()}")
                else:
                    logger.info(f"‚úÖ Processing task completed for {session_id}")
            except:
                pass
        
        processing_task.add_done_callback(task_done_callback)
        
        # Return success response
        return DocumentUploadResponse(
            document_id=session_id,
            filename=clean_filename,
            status="processing",
            message=f"Document uploaded successfully! Processing will take approximately {estimated_time} seconds. Thumbnails will be generated for quick preview.",
            file_size_mb=round(file_size_mb, 2),
            metadata=initial_metadata
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload failed: {str(e)}")
        logger.error(traceback.format_exc())
        
        # Try to cleanup
        try:
            if 'session_id' in locals():
                await storage_service.delete_blob(
                    settings.AZURE_CONTAINER_NAME, 
                    f"{session_id}.pdf"
                )
        except:
            pass
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=f"Upload failed: {str(e)}"
        )

# --- STATUS & INFO ENDPOINTS ---

@blueprint_router.get(
    "/documents/{document_id}/status",
    summary="Check document processing status",
    description="Check if a document has finished processing"
)
async def check_document_status(
    request: Request,
    document_id: str
):
    """Check the processing status of a document with detailed progress information."""
    clean_document_id = validate_document_id(document_id)
    
    storage_service = getattr(request.app.state, 'storage_service', None)
    if not storage_service:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail="Storage service unavailable"
        )
    
    try:
        # Check if actively processing
        async with processing_lock:
            if clean_document_id in processing_state:
                state = processing_state[clean_document_id]
                return JSONResponse(
                    content={
                        "document_id": clean_document_id,
                        "status": state.get('status', 'processing'),
                        "progress": state.get('progress', 0),
                        "message": f"Processing... Attempt {state.get('attempt', 1)}"
                    },
                    headers={"Cache-Control": "no-cache"}
                )
        
        cache_container = settings.AZURE_CACHE_CONTAINER_NAME
        status_blob = f"{clean_document_id}_status.json"
        
        # Check for status file
        if await storage_service.blob_exists(cache_container, status_blob):
            status_text = await storage_service.download_blob_as_text(
                container_name=cache_container,
                blob_name=status_blob
            )
            status_data = json.loads(status_text)
            
            # Calculate time remaining if processing
            if status_data.get('status') == 'processing':
                if 'started_at' in status_data:
                    started = datetime.fromisoformat(status_data['started_at'].rstrip('Z'))
                    elapsed = (datetime.utcnow() - started).total_seconds()
                    estimated_total = status_data.get('estimated_processing_time', 60)
                    remaining = max(0, int(estimated_total - elapsed))
                    status_data['estimated_time_remaining'] = remaining
                    status_data['progress_percentage'] = min(100, int((elapsed / estimated_total) * 100))
            
            return JSONResponse(
                content={
                    "document_id": clean_document_id,
                    "status": status_data.get('status', 'unknown'),
                    "message": status_data.get('message', ''),
                    "pages_processed": status_data.get('pages_processed'),
                    "total_pages": status_data.get('total_pages'),
                    "has_tables": status_data.get('has_tables', False),
                    "has_thumbnails": status_data.get('has_thumbnails', True),
                    "error": status_data.get('error'),
                    "uploaded_at": status_data.get('uploaded_at'),
                    "completed_at": status_data.get('completed_at'),
                    "estimated_time_remaining": status_data.get('estimated_time_remaining'),
                    "progress_percentage": status_data.get('progress_percentage'),
                    "retry_available": status_data.get('retry_available', False),
                    "processing_method": status_data.get('processing_method', 'standard')
                },
                headers={"Cache-Control": "no-cache"}
            )
        
        # Check if context exists (document is ready)
        context_blob = f"{clean_document_id}_context.txt"
        if await storage_service.blob_exists(cache_container, context_blob):
            return JSONResponse(
                content={
                    "document_id": clean_document_id,
                    "status": "ready",
                    "message": "Document is ready for analysis",
                    "has_thumbnails": True
                },
                headers={"Cache-Control": "public, max-age=3600"}
            )
        
        # Check if PDF exists
        main_container = settings.AZURE_CONTAINER_NAME
        if await storage_service.blob_exists(main_container, f"{clean_document_id}.pdf"):
            return JSONResponse(
                content={
                    "document_id": clean_document_id,
                    "status": "uploaded",
                    "message": "Document uploaded, processing pending"
                },
                headers={"Cache-Control": "no-cache"}
            )
        
        # Not found
        return JSONResponse(
            content={
                "document_id": clean_document_id,
                "status": "not_found",
                "message": "Document not found"
            },
            status_code=status.HTTP_404_NOT_FOUND
        )
        
    except Exception as e:
        logger.error(f"Status check failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to check status: {str(e)}"
        )

@blueprint_router.get(
    "/documents/{document_id}/info",
    response_model=DocumentInfoResponse,
    summary="Get document information",
    description="Get processing status and metadata for a specific document"
)
async def get_document_info(request: Request, document_id: str):
    """Get comprehensive information about a document including collaboration stats."""
    clean_document_id = validate_document_id(document_id)
    
    storage_service = getattr(request.app.state, 'storage_service', None)
    if not storage_service:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail="Storage service unavailable"
        )
    
    try:
        main_container = settings.AZURE_CONTAINER_NAME
        cache_container = settings.AZURE_CACHE_CONTAINER_NAME
        
        # Check if PDF exists
        pdf_exists = await storage_service.blob_exists(
            container_name=main_container,
            blob_name=f"{clean_document_id}.pdf"
        )
        
        # Check if context exists
        context_exists = await storage_service.blob_exists(
            container_name=cache_container,
            blob_name=f"{clean_document_id}_context.txt"
        )
        
        if not pdf_exists and not context_exists:
            return DocumentInfoResponse(
                document_id=clean_document_id,
                status="not_found",
                message="Document not found",
                exists=False
            )
        
        # Load status data
        status_data = None
        status_blob = f"{clean_document_id}_status.json"
        if await storage_service.blob_exists(cache_container, status_blob):
            status_text = await storage_service.download_blob_as_text(
                container_name=cache_container,
                blob_name=status_blob
            )
            status_data = json.loads(status_text)
        
        # Load metadata
        metadata = None
        if context_exists:
            metadata = await get_document_metadata(storage_service, clean_document_id)
        
        # Calculate collaboration stats
        published_notes = 0
        active_collaborators = set()
        recent_activity = False
        
        try:
            notes_blob = f"{clean_document_id}_notes.json"
            if await storage_service.blob_exists(cache_container, notes_blob):
                notes_text = await storage_service.download_blob_as_text(
                    container_name=cache_container,
                    blob_name=notes_blob
                )
                all_notes = json.loads(notes_text)
                
                for note in all_notes:
                    if not note.get('is_private', True):
                        published_notes += 1
                        active_collaborators.add(note.get('author'))
                        
                        if note.get('timestamp'):
                            note_time = datetime.fromisoformat(note['timestamp'].rstrip('Z'))
                            if (datetime.utcnow() - note_time).total_seconds() < 86400:
                                recent_activity = True
        except Exception as e:
            logger.warning(f"Failed to load collaboration stats: {e}")
        
        # Determine status and message
        if status_data:
            status_value = status_data.get('status', 'unknown')
            message = status_data.get('message', '')
            
            if status_value == 'error':
                message = f"Processing failed: {status_data.get('error', 'Unknown error')}"
            elif status_value == 'processing':
                message = "Document is being processed"
            elif status_value == 'ready':
                page_count = metadata.get('page_count', 0) if metadata else 0
                message = f"Document ready for analysis. {page_count} pages processed with thumbnails."
        else:
            if context_exists and metadata:
                status_value = "ready"
                message = f"Document ready. {metadata.get('page_count', 0)} pages available."
            elif pdf_exists:
                status_value = "uploaded"
                message = "Document uploaded, processing pending"
            else:
                status_value = "partial"
                message = "Document partially processed"
        
        # Get session info
        session_info = None
        session_service = getattr(request.app.state, 'session_service', None)
        if session_service:
            try:
                session_info = await session_service.get_session_info(clean_document_id)
            except:
                pass
        
        # Get image settings
        image_info = None
        if metadata:
            image_info = metadata.get('image_settings', {
                'high_quality': {
                    'dpi': 150,
                    'quality': 90,
                    'progressive': True
                },
                'thumbnail': {
                    'dpi': 72,
                    'quality': 75
                }
            })
        
        return DocumentInfoResponse(
            document_id=clean_document_id,
            status=status_value,
            message=message,
            exists=True,
            metadata={
                "filename": status_data.get('filename') if status_data else None,
                "author": status_data.get('author') if status_data else None,
                "uploaded_at": status_data.get('uploaded_at') if status_data else None,
                "file_size_mb": status_data.get('file_size_mb') if status_data else None,
                "page_count": metadata.get('page_count') if metadata else None,
                "total_pages": metadata.get('total_pages') if metadata else None,
                "processing_time": metadata.get('processing_time') if metadata else None,
                "grid_systems_detected": metadata.get('grid_systems_detected', 0) if metadata else None,
                "has_text": metadata.get('extraction_summary', {}).get('has_text', False) if metadata else None,
                "has_tables": metadata.get('extraction_summary', {}).get('has_tables', False) if metadata else None,
                "has_thumbnails": metadata.get('extraction_summary', {}).get('has_thumbnails', True) if metadata else None,
                "drawing_types": list(metadata.get('drawing_types', {}).keys()) if metadata else None,
                "image_settings": image_info
            },
            total_published_notes=published_notes,
            active_collaborators=len(active_collaborators),
            recent_public_activity=recent_activity,
            session_info=session_info
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get document info: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail="Failed to retrieve document information"
        )

# --- DOWNLOAD ENDPOINT ---

@blueprint_router.get("/documents/{document_id}/download")
async def download_document_pdf(
    request: Request,
    document_id: str
):
    """Download the original PDF file for viewing with streaming support."""
    clean_document_id = validate_document_id(document_id)
    
    storage_service = getattr(request.app.state, 'storage_service', None)
    if not storage_service:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail="Storage service unavailable"
        )
    
    try:
        container_name = settings.AZURE_CONTAINER_NAME
        blob_name = f"{clean_document_id}.pdf"
        
        # Check if exists
        if not await storage_service.blob_exists(container_name, blob_name):
            logger.warning(f"PDF not found: {blob_name}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Document {clean_document_id} not found"
            )
        
        logger.info(f"üì• Downloading PDF: {blob_name}")
        
        # Download PDF
        pdf_bytes = await storage_service.download_blob_as_bytes(
            container_name=container_name,
            blob_name=blob_name
        )
        
        logger.info(f"‚úÖ PDF downloaded: {len(pdf_bytes)} bytes")
        
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"inline; filename=\"{clean_document_id}.pdf\"",
                "Content-Length": str(len(pdf_bytes)),
                "Cache-Control": "public, max-age=3600",
                "Accept-Ranges": "bytes"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Download failed for {clean_document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail=f"Download failed: {str(e)}"
        )

# --- CHAT ENDPOINT ---

@blueprint_router.post(
    "/documents/{document_id}/chat",
    response_model=ChatResponse,
    summary="Chat with a blueprint",
    description="Send a question about an uploaded blueprint and receive AI analysis"
)
async def chat_with_document(
    request: Request,
    document_id: str,
    chat_request: ChatRequest
):
    """Chat with an uploaded document using AI analysis."""
    clean_document_id = validate_document_id(document_id)
    
    if not chat_request.prompt or len(chat_request.prompt.strip()) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail="Prompt cannot be empty"
        )
    
    prompt = chat_request.prompt.strip()[:2000]
    author = re.sub(r'[^\w\s\-_@.]', '', chat_request.author)[:100]
    
    ai_service = getattr(request.app.state, 'ai_service', None)
    storage_service = getattr(request.app.state, 'storage_service', None)
    
    if not ai_service:
        logger.warning("‚ö†Ô∏è AI service not available")
        # Return a helpful response instead of error
        return ChatResponse(
            session_id=clean_document_id,
            ai_response="AI service is temporarily unavailable. Your document has been uploaded successfully and you can download it anytime.",
            visual_highlights=[],
            current_page=chat_request.current_page,
            query_session_id=str(uuid.uuid4()),
            all_highlight_pages={},
            trade_summary={},
            note_suggestion=None
        )
    
    if not storage_service:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail="Storage service is not available. Please try again later."
        )
    
    try:
        logger.info(f"üí¨ Chat request for {clean_document_id} from {author}")
        logger.info(f"   Prompt: {prompt[:100]}...")
        
        cache_container = settings.AZURE_CACHE_CONTAINER_NAME
        
        # Check document status
        status_blob = f"{clean_document_id}_status.json"
        if await storage_service.blob_exists(cache_container, status_blob):
            status_text = await storage_service.download_blob_as_text(
                container_name=cache_container,
                blob_name=status_blob
            )
            status_data = json.loads(status_text)
            
            if status_data.get('status') == 'processing':
                estimated_remaining = 30
                if status_data.get('started_at'):
                    started = datetime.fromisoformat(status_data['started_at'].rstrip('Z'))
                    elapsed = (datetime.utcnow() - started).total_seconds()
                    estimated_total = status_data.get('estimated_processing_time', 60)
                    estimated_remaining = max(5, int(estimated_total - elapsed))
                
                raise HTTPException(
                    status_code=status.HTTP_425_TOO_EARLY,
                    detail=f"Document is still processing. Estimated time remaining: {estimated_remaining} seconds.",
                    headers={"Retry-After": str(estimated_remaining)}
                )
            
            elif status_data.get('status') == 'error':
                # Return helpful response even on error
                if status_data.get('retry_available', False):
                    return ChatResponse(
                        session_id=clean_document_id,
                        ai_response="Document processing encountered an issue but your file is safe. Please try re-uploading or download the original PDF.",
                        visual_highlights=[],
                        current_page=chat_request.current_page,
                        query_session_id=str(uuid.uuid4()),
                        all_highlight_pages={},
                        trade_summary={},
                        note_suggestion=None
                    )
        
        # Check if context exists
        context_blob = f"{clean_document_id}_context.txt"
        if not await storage_service.blob_exists(cache_container, context_blob):
            main_container = settings.AZURE_CONTAINER_NAME
            if await storage_service.blob_exists(main_container, f"{clean_document_id}.pdf"):
                raise HTTPException(
                    status_code=status.HTTP_425_TOO_EARLY,
                    detail="Document is pending processing. Please try again in a few moments.",
                    headers={"Retry-After": "30"}
                )
            else:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Document '{clean_document_id}' not found"
                )
        
        try:
            # Get AI response - no timeout for testing
            logger.info(f"ü§ñ Calling AI service for {clean_document_id}")
            
            ai_result = await ai_service.get_ai_response(
                prompt=prompt,
                document_id=clean_document_id,
                storage_service=storage_service,
                author=author,
                current_page=chat_request.current_page,
                request_highlights=True,
                reference_previous=chat_request.reference_previous,
                preserve_existing=chat_request.preserve_existing,
                show_trade_info=chat_request.show_trade_info,
                detect_conflicts=chat_request.detect_conflicts,
                auto_suggest_notes=chat_request.auto_suggest_notes,
                note_suggestion_threshold=chat_request.note_suggestion_threshold
            )
            
            logger.info(f"‚úÖ AI response received")
            
        except Exception as e:
            logger.error(f"AI service error: {e}")
            # Return generic helpful response
            return ChatResponse(
                session_id=clean_document_id,
                ai_response="I encountered an issue analyzing the document. Please try rephrasing your question or asking about a specific page.",
                visual_highlights=[],
                current_page=chat_request.current_page,
                query_session_id=str(uuid.uuid4()),
                all_highlight_pages={},
                trade_summary={},
                note_suggestion=None
            )
        
        # Save chat history
        await save_chat_to_history(
            document_id=clean_document_id,
            author=author,
            prompt=prompt,
            response=ai_result.get("ai_response", ""),
            storage_service=storage_service,
            metadata={
                "has_highlights": bool(ai_result.get("visual_highlights")),
                "note_suggested": bool(ai_result.get("note_suggestion")),
                "processing_time": ai_result.get("processing_time"),
                "pages_analyzed": len(ai_result.get("source_pages", []))
            }
        )
        
        # Update session
        session_service = getattr(request.app.state, 'session_service', None)
        if session_service:
            try:
                await session_service.record_chat_activity(
                    document_id=clean_document_id,
                    user=author,
                    chat_data={
                        "timestamp": datetime.utcnow().isoformat() + "Z",
                        "author": author,
                        "prompt": prompt[:200],
                        "has_response": bool(ai_result.get("ai_response"))
                    }
                )
            except Exception as e:
                logger.warning(f"Session activity update failed: {e}")
        
        response = ChatResponse(
            session_id=clean_document_id,
            ai_response=ai_result.get("ai_response", "I was unable to analyze the blueprint. Please try rephrasing your question."),
            visual_highlights=ai_result.get("visual_highlights"),
            current_page=chat_request.current_page,
            query_session_id=ai_result.get("query_session_id"),
            all_highlight_pages=ai_result.get("all_highlight_pages"),
            trade_summary=ai_result.get("trade_summary"),
            note_suggestion=ai_result.get("note_suggestion")
        )
        
        logger.info(f"‚úÖ Chat response generated successfully")
        if ai_result.get("visual_highlights"):
            logger.info(f"   Highlights: {len(ai_result['visual_highlights'])}")
        if ai_result.get("note_suggestion"):
            logger.info(f"   Note suggested: {ai_result['note_suggestion'].id}")
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat processing failed: {str(e)}")
        logger.error(traceback.format_exc())
        
        # Return helpful response instead of error
        return ChatResponse(
            session_id=clean_document_id,
            ai_response="An unexpected error occurred. Your document is safe and you can still download it.",
            visual_highlights=[],
            current_page=chat_request.current_page,
            query_session_id=str(uuid.uuid4()),
            all_highlight_pages={},
            trade_summary={},
            note_suggestion=None
        )

# --- QUICK NOTE CREATION ---

@blueprint_router.post("/documents/{document_id}/notes/quick-create")
async def quick_create_note_from_suggestion(
    request: Request,
    document_id: str,
    quick_note: QuickNoteCreate
):
    """Quick endpoint to create a note from AI suggestion with validation."""
    clean_document_id = validate_document_id(document_id)
    
    storage_service = getattr(request.app.state, 'storage_service', None)
    if not storage_service:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
            detail="Storage service unavailable"
        )
    
    try:
        cache_container = settings.AZURE_CACHE_CONTAINER_NAME
        notes_blob = f"{clean_document_id}_notes.json"
        
        # Load existing notes
        all_notes = []
        try:
            notes_text = await storage_service.download_blob_as_text(
                container_name=cache_container,
                blob_name=notes_blob
            )
            all_notes = json.loads(notes_text)
        except FileNotFoundError:
            logger.info(f"Creating first note for document {clean_document_id}")
        except json.JSONDecodeError:
            logger.error(f"Corrupted notes file for {clean_document_id}, starting fresh")
            all_notes = []
        
        # Validate limits
        max_notes = settings.MAX_NOTES_PER_DOCUMENT
        user_notes = [n for n in all_notes if n.get('author') == quick_note.author]
        
        if len(user_notes) >= max_notes:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Note limit reached. Maximum {max_notes} notes per user per document."
            )
        
        total_chars = sum(len(n.get('text', '')) for n in all_notes)
        if total_chars + len(quick_note.text) > settings.MAX_TOTAL_NOTE_CHARS:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Document note character limit exceeded."
            )
        
        # Create new note
        new_note = {
            "note_id": f"note_{uuid.uuid4().hex[:8]}",
            "document_id": clean_document_id,
            "text": quick_note.text[:settings.MAX_NOTE_LENGTH],
            "note_type": quick_note.note_type.value,
            "author": quick_note.author,
            "impacts_trades": quick_note.impacts_trades[:10],
            "priority": quick_note.priority.value,
            "is_private": quick_note.is_private,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "char_count": len(quick_note.text),
            "status": "open",
            "ai_suggested": quick_note.ai_suggested,
            "suggestion_confidence": quick_note.suggestion_confidence,
            "related_query_sessions": quick_note.related_query_sessions[:10],
            "related_element_ids": quick_note.related_highlights[:20],
            "source_pages": sorted(list(set(quick_note.source_pages)))[:50]
        }
        
        all_notes.append(new_note)
        
        # Save updated notes
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=notes_blob,
            data=json.dumps(all_notes, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
        logger.info(f"‚úÖ Note created: {new_note['note_id']} (AI: {quick_note.ai_suggested})")
        
        return {
            "status": "success",
            "note_id": new_note["note_id"],
            "message": "Note created successfully",
            "note_details": {
                "type": quick_note.note_type.value,
                "priority": quick_note.priority.value,
                "is_private": quick_note.is_private,
                "ai_suggested": quick_note.ai_suggested
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Note creation failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, 
            detail="Failed to create note. Please try again."
        )

# --- SERVICE HEALTH CHECK ---

@blueprint_router.get("/health/services", include_in_schema=False)
async def check_services(request: Request):
    """Check health status of all services."""
    def check_service(service_name: str) -> Dict[str, Any]:
        service = getattr(request.app.state, service_name, None)
        
        if service is None:
            return {
                "available": False, 
                "status": "not_initialized"
            }
        
        health_info = {"available": True, "status": "healthy"}
        
        try:
            if hasattr(service, 'get_connection_info'):
                health_info["connection_info"] = service.get_connection_info()
            elif hasattr(service, 'get_service_statistics'):
                health_info["statistics"] = service.get_service_statistics()
            elif hasattr(service, 'is_running'):
                health_info["is_running"] = service.is_running()
        except Exception as e:
            health_info["status"] = "error"
            health_info["error"] = str(e)
        
        return health_info
    
    async with processing_lock:
        active_processing = len(processing_state)
    
    return {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "services": {
            "storage": check_service('storage_service'),
            "ai": check_service('ai_service'),
            "pdf": check_service('pdf_service'),
            "session": check_service('session_service')
        },
        "processing": {
            "active_tasks": active_processing,
            "task_ids": list(processing_state.keys()) if active_processing > 0 else []
        },
        "configuration": {
            "max_file_size_mb": settings.MAX_FILE_SIZE_MB,
            "pdf_max_pages": settings.PDF_MAX_PAGES,
            "unlimited_loading": settings.UNLIMITED_PAGE_LOADING,
            "image_generation": {
                "thumbnails_enabled": True,
                "high_quality_enabled": True,
                "thumbnail_dpi": settings.PDF_THUMBNAIL_DPI,
                "high_quality_dpi": 150
            }
        }
    }

# --- Helper Functions ---

async def save_chat_to_history(
    document_id: str,
    author: str,
    prompt: str,
    response: str,
    storage_service,
    metadata: Dict[str, Any] = None
):
    """Save chat interaction to history with metadata."""
    try:
        cache_container = settings.AZURE_CACHE_CONTAINER_NAME
        chat_blob = f"{document_id}_all_chats.json"
        
        # Load existing history
        chat_history = []
        try:
            chat_data = await storage_service.download_blob_as_text(
                container_name=cache_container,
                blob_name=chat_blob
            )
            chat_history = json.loads(chat_data)
        except:
            pass
        
        # Add new entry
        chat_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "author": author,
            "prompt": prompt[:500],
            "response": response[:1000],
            "metadata": metadata or {}
        }
        
        chat_history.append(chat_entry)
        
        # Limit history size
        max_chats = settings.MAX_CHAT_LOGS
        if len(chat_history) > max_chats:
            chat_history = chat_history[-max_chats:]
        
        # Save updated history
        await storage_service.upload_file(
            container_name=cache_container,
            blob_name=chat_blob,
            data=json.dumps(chat_history, indent=2).encode('utf-8'),
            content_type="application/json"
        )
        
    except Exception as e:
        logger.warning(f"Failed to save chat history: {e}")