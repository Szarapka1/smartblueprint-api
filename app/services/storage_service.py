# app/services/storage_service.py - PRODUCTION-READY BULLETPROOF VERSION

import logging
import asyncio
import json
import time
import os
from typing import List, Optional, Dict, Any, AsyncGenerator, Tuple, Union
from datetime import datetime, timedelta
import re
from contextlib import asynccontextmanager
from functools import wraps
import hashlib

from app.core.config import get_settings, CONFIG
from azure.storage.blob.aio import BlobServiceClient, ContainerClient, BlobClient
from azure.core.exceptions import ResourceExistsError, ResourceNotFoundError, AzureError, ClientAuthenticationError, HttpResponseError
from azure.storage.blob import BlobPrefix, ContentSettings, BlobProperties
from azure.core.pipeline.transport import AsyncioRequestsTransport

logger = logging.getLogger(__name__)

# --- Decorators for retry and timeout ---

def with_timeout(timeout_seconds: float):
    """Decorator to add timeout to async functions"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            try:
                return await asyncio.wait_for(func(*args, **kwargs), timeout=timeout_seconds)
            except asyncio.TimeoutError:
                logger.error(f"‚è∞ Timeout in {func.__name__} after {timeout_seconds}s")
                raise TimeoutError(f"Operation {func.__name__} timed out after {timeout_seconds}s")
        return wrapper
    return decorator

def with_retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """Decorator to add retry logic to async functions"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_attempts):
                try:
                    return await func(*args, **kwargs)
                except (ResourceNotFoundError, ClientAuthenticationError) as e:
                    # Don't retry on these errors
                    raise
                except Exception as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        wait_time = delay * (backoff ** attempt)
                        logger.warning(f"üîÑ Retry {attempt + 1}/{max_attempts} for {func.__name__} after {wait_time}s: {e}")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"‚ùå All {max_attempts} attempts failed for {func.__name__}")
            raise last_exception
        return wrapper
    return decorator

class StorageService:
    """
    Production-grade Azure Blob Storage service with maximum reliability
    
    Features:
    - Extended timeouts for testing large files
    - Comprehensive retry logic with exponential backoff
    - Connection pooling and reuse
    - Detailed error handling and recovery
    - Progress tracking for large operations
    - Automatic connection recovery
    - Memory-efficient streaming
    """
    
    def __init__(self, settings=None):
        self.settings = settings or get_settings()
        
        if not self.settings.AZURE_STORAGE_CONNECTION_STRING:
            raise ValueError("AZURE_STORAGE_CONNECTION_STRING is required")
        
        # Thread safety
        self._lock = asyncio.Lock()
        self._initialization_lock = asyncio.Lock()
        self._initialized = False
        
        # Container client cache
        self._container_clients: Dict[str, ContainerClient] = {}
        self._blob_clients: Dict[str, BlobClient] = {}
        
        # Connection health tracking
        self._last_successful_operation = time.time()
        self._consecutive_failures = 0
        self._connection_healthy = True
        
        # Performance tracking
        self._operation_stats = {
            "uploads": {"count": 0, "bytes": 0, "errors": 0},
            "downloads": {"count": 0, "bytes": 0, "errors": 0},
            "deletes": {"count": 0, "errors": 0},
            "lists": {"count": 0, "errors": 0}
        }
        
        # Extended timeout configuration for testing
        self.timeout_config = {
            "connection_timeout": 300,      # 5 minutes
            "read_timeout": 3600,          # 1 hour for large downloads
            "write_timeout": 3600,         # 1 hour for large uploads
            "operation_timeout": 1800,     # 30 minutes per operation
            "total_timeout": 7200,         # 2 hours total
            "retry_timeout": 300,          # 5 minutes for retries
            "initialization_timeout": 120   # 2 minutes for init
        }
        
        # Retry configuration
        self.retry_config = {
            "max_retries": 5,
            "initial_delay": 1.0,
            "max_delay": 60.0,
            "backoff_factor": 2.0,
            "retry_on_timeout": True,
            "retry_on_server_error": True
        }
        
        # Chunk sizes for optimal performance
        self.chunk_config = {
            "download_chunk_size": 32 * 1024 * 1024,    # 32MB chunks
            "upload_chunk_size": 16 * 1024 * 1024,      # 16MB chunks
            "stream_chunk_size": 4 * 1024 * 1024,       # 4MB for streaming
            "max_single_put_size": 256 * 1024 * 1024,   # 256MB single upload
            "max_single_get_size": 256 * 1024 * 1024    # 256MB single download
        }
        
        logger.info("üöÄ StorageService initializing with extended timeouts...")
        logger.info(f"‚è∞ Timeouts: Connection={self.timeout_config['connection_timeout']}s, "
                   f"Read={self.timeout_config['read_timeout']}s, "
                   f"Operation={self.timeout_config['operation_timeout']}s")
        logger.info(f"üîÑ Retry: Max={self.retry_config['max_retries']} attempts with exponential backoff")

    async def _initialize_client(self):
        """Initialize blob service client with proper error handling"""
        async with self._initialization_lock:
            if self._initialized:
                return
            
            try:
                logger.info("üîß Initializing Azure Blob Storage client...")
                
                # Create transport with extended timeouts
                transport = AsyncioRequestsTransport(
                    connection_timeout=self.timeout_config['connection_timeout'],
                    read_timeout=self.timeout_config['read_timeout']
                )
                
                # Create blob service client with all optimizations
                self.blob_service_client = BlobServiceClient.from_connection_string(
                    self.settings.AZURE_STORAGE_CONNECTION_STRING,
                    max_single_get_size=self.chunk_config['max_single_get_size'],
                    max_chunk_get_size=self.chunk_config['download_chunk_size'],
                    max_single_put_size=self.chunk_config['max_single_put_size'],
                    max_block_size=self.chunk_config['upload_chunk_size'],
                    transport=transport
                )
                
                # Test connection
                await self._test_connection()
                
                self._initialized = True
                self._connection_healthy = True
                logger.info("‚úÖ StorageService initialized successfully")
                
            except Exception as e:
                self._connection_healthy = False
                logger.error(f"‚ùå Failed to initialize Azure Storage client: {e}")
                raise ConnectionError(f"Azure Storage initialization failed: {e}")

    @with_timeout(60.0)
    async def _test_connection(self):
        """Test connection with timeout"""
        try:
            account_info = await self.blob_service_client.get_account_information()
            logger.info(f"‚úÖ Connected to Azure Storage Account: {account_info.get('account_kind', 'Unknown')}")
            
            # Extract account name from connection string
            conn_str = self.settings.AZURE_STORAGE_CONNECTION_STRING
            if "AccountName=" in conn_str:
                account_match = re.search(r'AccountName=([^;]+)', conn_str)
                if account_match:
                    logger.info(f"üìç Account Name: {account_match.group(1)}")
            
            self._last_successful_operation = time.time()
            self._consecutive_failures = 0
            
        except Exception as e:
            logger.error(f"‚ùå Connection test failed: {e}")
            raise

    async def _ensure_initialized(self):
        """Ensure client is initialized before operations"""
        if not self._initialized:
            await self._initialize_client()

    async def _get_container_client(self, container_name: str) -> ContainerClient:
        """Get or create a cached container client with validation"""
        await self._ensure_initialized()
        
        if not container_name:
            raise ValueError("Container name cannot be empty")
        
        # Validate container name (Azure rules)
        if not re.match(r'^[a-z0-9]([a-z0-9-]*[a-z0-9])?$', container_name):
            raise ValueError(f"Invalid container name: {container_name}. Must be lowercase alphanumeric with hyphens.")
        
        if container_name not in self._container_clients:
            self._container_clients[container_name] = self.blob_service_client.get_container_client(container_name)
            logger.debug(f"üì¶ Created container client for: {container_name}")
            
        return self._container_clients[container_name]

    def _validate_blob_name(self, blob_name: str) -> str:
        """Comprehensive blob name validation"""
        if not blob_name or not isinstance(blob_name, str):
            raise ValueError("Blob name must be a non-empty string")
        
        # Remove dangerous path traversal attempts
        blob_name = blob_name.replace("../", "").replace("..\\", "")
        blob_name = blob_name.replace("./", "").replace(".\\", "")
        
        # Remove leading/trailing whitespace and slashes
        blob_name = blob_name.strip().strip("/\\")
        
        # Check length (Azure limit is 1024 characters)
        if len(blob_name) > 1024:
            raise ValueError(f"Blob name too long: {len(blob_name)} characters (max 1024)")
        
        if len(blob_name) == 0:
            raise ValueError("Blob name cannot be empty after sanitization")
        
        # Allow more characters for flexibility but log warnings
        # Azure allows most UTF-8 characters except for some reserved ones
        reserved_chars = ['#', '?', '%']
        for char in reserved_chars:
            if char in blob_name:
                logger.warning(f"‚ö†Ô∏è Blob name contains reserved character '{char}': {blob_name}")
        
        # Check for valid path segments
        segments = blob_name.split('/')
        for segment in segments:
            if not segment or segment in ['.', '..']:
                raise ValueError(f"Invalid path segment in blob name: {blob_name}")
        
        logger.debug(f"‚úÖ Validated blob name: {blob_name}")
        return blob_name

    def _get_content_type(self, blob_name: str) -> str:
        """Determine content type from blob name"""
        ext = os.path.splitext(blob_name.lower())[1]
        content_types = {
            '.pdf': 'application/pdf',
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.json': 'application/json',
            '.txt': 'text/plain',
            '.xml': 'application/xml',
            '.csv': 'text/csv',
            '.html': 'text/html',
            '.gz': 'application/gzip',
            '.zip': 'application/zip'
        }
        return content_types.get(ext, 'application/octet-stream')

    async def _handle_operation_error(self, error: Exception, operation: str):
        """Central error handling with connection health tracking"""
        self._consecutive_failures += 1
        self._operation_stats.get(operation, {}).get("errors", 0)
        
        if self._consecutive_failures > 5:
            self._connection_healthy = False
            logger.error(f"‚ö†Ô∏è Connection appears unhealthy after {self._consecutive_failures} failures")
        
        # Log detailed error information
        if isinstance(error, HttpResponseError):
            logger.error(f"‚ùå Azure HTTP Error in {operation}: {error.status_code} - {error.message}")
        elif isinstance(error, ClientAuthenticationError):
            logger.error(f"‚ùå Authentication Error in {operation}: {error}")
        elif isinstance(error, TimeoutError):
            logger.error(f"‚è∞ Timeout Error in {operation}: {error}")
        else:
            logger.error(f"‚ùå Error in {operation}: {type(error).__name__} - {error}")

    async def _track_successful_operation(self, operation: str, bytes_processed: int = 0):
        """Track successful operations"""
        self._last_successful_operation = time.time()
        self._consecutive_failures = 0
        self._connection_healthy = True
        
        if operation in self._operation_stats:
            self._operation_stats[operation]["count"] += 1
            if bytes_processed > 0:
                self._operation_stats[operation]["bytes"] += bytes_processed

    # === MAIN OPERATIONS ===

    async def verify_connection(self):
        """Verify connection with comprehensive checks"""
        async with self._lock:
            logger.info("üîç Verifying Azure Storage connection...")
            
            try:
                await self._ensure_initialized()
                
                # Test account access
                await self._test_connection()
                
                # Test container access for configured containers
                containers_to_test = []
                if hasattr(self.settings, 'AZURE_CONTAINER_NAME') and self.settings.AZURE_CONTAINER_NAME:
                    containers_to_test.append(self.settings.AZURE_CONTAINER_NAME)
                if hasattr(self.settings, 'AZURE_CACHE_CONTAINER_NAME') and self.settings.AZURE_CACHE_CONTAINER_NAME:
                    containers_to_test.append(self.settings.AZURE_CACHE_CONTAINER_NAME)
                
                for container_name in set(containers_to_test):  # Remove duplicates
                    try:
                        container_client = await self._get_container_client(container_name)
                        
                        # Try to get container properties with timeout
                        properties = await asyncio.wait_for(
                            container_client.get_container_properties(),
                            timeout=30.0
                        )
                        
                        logger.info(f"‚úÖ Container '{container_name}' accessible")
                        logger.debug(f"   Last modified: {properties.get('last_modified', 'Unknown')}")
                        
                    except ResourceNotFoundError:
                        logger.warning(f"‚ö†Ô∏è Container '{container_name}' not found - will create if needed")
                        
                        # Optionally create the container
                        try:
                            await container_client.create_container()
                            logger.info(f"‚úÖ Created container '{container_name}'")
                        except ResourceExistsError:
                            pass  # Container was created by another process
                        except Exception as e:
                            logger.error(f"‚ùå Failed to create container '{container_name}': {e}")
                    
                    except Exception as e:
                        logger.error(f"‚ùå Cannot access container '{container_name}': {e}")
                
                logger.info("‚úÖ Storage connection verified successfully")
                
            except Exception as e:
                await self._handle_operation_error(e, "verify_connection")
                raise ConnectionError(f"Storage connection verification failed: {e}")

    @with_retry(max_attempts=5, delay=1.0, backoff=2.0)
    async def upload_file(self, container_name: str, blob_name: str, data: bytes, 
                         content_type: Optional[str] = None, metadata: Optional[Dict[str, str]] = None) -> str:
        """Upload file with maximum reliability"""
        start_time = time.time()
        blob_name = self._validate_blob_name(blob_name)
        
        if not isinstance(data, bytes):
            raise ValueError("Data must be bytes")
        
        if len(data) == 0:
            logger.warning(f"‚ö†Ô∏è Attempting to upload empty data to {blob_name}")
            # Allow empty files but warn
        
        file_size_mb = len(data) / (1024 * 1024)
        logger.info(f"üì§ Uploading {blob_name} ({file_size_mb:.2f}MB) to {container_name}")
        
        try:
            container_client = await self._get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob=blob_name)
            
            # Determine content type
            if not content_type:
                content_type = self._get_content_type(blob_name)
            
            # Prepare upload options
            upload_options = {
                'data': data,
                'overwrite': True,
                'content_settings': ContentSettings(content_type=content_type),
                'metadata': metadata or {},
                'validate_content': False  # Skip MD5 validation for speed
            }
            
            # For large files, use concurrent upload
            if file_size_mb > 64:
                upload_options['max_concurrency'] = min(16, max(2, int(file_size_mb / 32)))
                logger.debug(f"üöÄ Using {upload_options['max_concurrency']} concurrent connections")
            
            # Upload with timeout based on file size
            timeout = max(300, file_size_mb * 10)  # At least 5 minutes, or 10s per MB
            
            await asyncio.wait_for(
                blob_client.upload_blob(**upload_options),
                timeout=timeout
            )
            
            elapsed = time.time() - start_time
            speed_mbps = (file_size_mb * 8) / elapsed if elapsed > 0 else 0
            
            logger.info(f"‚úÖ Uploaded {blob_name} in {elapsed:.1f}s @ {speed_mbps:.1f}Mbps")
            
            await self._track_successful_operation("uploads", len(data))
            
            return blob_client.url
            
        except asyncio.TimeoutError:
            await self._handle_operation_error(TimeoutError(f"Upload timeout after {timeout}s"), "uploads")
            raise TimeoutError(f"Upload of {blob_name} timed out after {timeout}s")
        except Exception as e:
            await self._handle_operation_error(e, "uploads")
            raise RuntimeError(f"Failed to upload {blob_name}: {e}")

    @with_retry(max_attempts=5, delay=1.0, backoff=2.0)
    async def download_blob_as_bytes(self, container_name: str, blob_name: str) -> bytes:
        """Download blob with maximum reliability"""
        start_time = time.time()
        blob_name = self._validate_blob_name(blob_name)
        
        logger.info(f"üì• Downloading {blob_name} from {container_name}")
        
        try:
            container_client = await self._get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob=blob_name)
            
            # Get blob properties first to know the size
            try:
                properties = await blob_client.get_blob_properties()
                blob_size_mb = properties.size / (1024 * 1024)
                logger.debug(f"üìä Blob size: {blob_size_mb:.2f}MB")
            except:
                blob_size_mb = 0
            
            # Download with appropriate timeout
            timeout = max(300, blob_size_mb * 5)  # At least 5 minutes, or 5s per MB
            
            download_stream = await asyncio.wait_for(
                blob_client.download_blob(
                    max_concurrency=8,
                    validate_content=False
                ),
                timeout=timeout
            )
            
            # Read all data
            data = await asyncio.wait_for(
                download_stream.readall(),
                timeout=timeout
            )
            
            elapsed = time.time() - start_time
            actual_size_mb = len(data) / (1024 * 1024)
            speed_mbps = (actual_size_mb * 8) / elapsed if elapsed > 0 else 0
            
            logger.info(f"‚úÖ Downloaded {blob_name} ({actual_size_mb:.2f}MB) in {elapsed:.1f}s @ {speed_mbps:.1f}Mbps")
            
            await self._track_successful_operation("downloads", len(data))
            
            return data
            
        except ResourceNotFoundError:
            logger.error(f"‚ùå Blob not found: {blob_name} in {container_name}")
            
            # Try to list similar blobs to help debugging
            try:
                prefix = blob_name.split('_')[0] if '_' in blob_name else blob_name[:10]
                similar = await self.list_blobs(container_name, prefix=prefix)
                if similar:
                    logger.info(f"üí° Did you mean one of these? {similar[:5]}")
            except:
                pass
            
            raise FileNotFoundError(f"Blob '{blob_name}' not found in container '{container_name}'")
            
        except asyncio.TimeoutError:
            await self._handle_operation_error(TimeoutError(f"Download timeout after {timeout}s"), "downloads")
            raise TimeoutError(f"Download of {blob_name} timed out after {timeout}s")
        except Exception as e:
            await self._handle_operation_error(e, "downloads")
            raise RuntimeError(f"Failed to download {blob_name}: {e}")

    async def download_blob_as_text(self, container_name: str, blob_name: str, 
                                   encoding: str = "utf-8") -> str:
        """Download blob as text with encoding detection"""
        blob_bytes = await self.download_blob_as_bytes(container_name, blob_name)
        
        # Try to decode with specified encoding
        try:
            return blob_bytes.decode(encoding)
        except UnicodeDecodeError:
            logger.warning(f"‚ö†Ô∏è Failed to decode {blob_name} with {encoding}")
            
            # Try common encodings
            encodings = ['utf-8-sig', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
            for alt_encoding in encodings:
                try:
                    logger.debug(f"üîÑ Trying {alt_encoding} encoding...")
                    return blob_bytes.decode(alt_encoding)
                except UnicodeDecodeError:
                    continue
            
            # Last resort - ignore errors
            logger.warning(f"‚ö†Ô∏è Using lossy decoding for {blob_name}")
            return blob_bytes.decode('utf-8', errors='ignore')

    async def download_blob_as_json(self, container_name: str, blob_name: str) -> Dict[str, Any]:
        """Download and parse JSON blob"""
        try:
            text = await self.download_blob_as_text(container_name, blob_name)
            
            # Try to parse JSON
            try:
                return json.loads(text)
            except json.JSONDecodeError as e:
                # Log first 200 chars of invalid JSON for debugging
                logger.error(f"‚ùå Invalid JSON in {blob_name}: {text[:200]}...")
                logger.error(f"   JSON Error: {e}")
                raise ValueError(f"Blob '{blob_name}' contains invalid JSON: {e}")
                
        except Exception as e:
            logger.error(f"Failed to download JSON blob {blob_name}: {e}")
            raise

    @with_retry(max_attempts=3, delay=0.5, backoff=2.0)
    async def list_blobs(self, container_name: str, prefix: str = "", 
                        suffix: str = "", max_results: Optional[int] = None) -> List[str]:
        """List blobs with filtering and pagination"""
        try:
            container_client = await self._get_container_client(container_name)
            
            blob_names = []
            count = 0
            
            # List blobs with prefix
            async for blob in container_client.list_blobs(name_starts_with=prefix):
                blob_name = blob.name
                
                # Apply suffix filter
                if suffix and not blob_name.endswith(suffix):
                    continue
                
                blob_names.append(blob_name)
                count += 1
                
                # Respect max_results
                if max_results and count >= max_results:
                    break
            
            # Sort for consistency
            blob_names.sort()
            
            logger.info(f"‚úÖ Listed {len(blob_names)} blobs from '{container_name}' "
                       f"(prefix='{prefix}', suffix='{suffix}')")
            
            await self._track_successful_operation("lists")
            
            return blob_names
            
        except Exception as e:
            await self._handle_operation_error(e, "lists")
            raise RuntimeError(f"Failed to list blobs: {e}")

    @with_retry(max_attempts=3, delay=0.5, backoff=2.0)
    async def blob_exists(self, container_name: str, blob_name: str) -> bool:
        """Check if blob exists with retry"""
        try:
            blob_name = self._validate_blob_name(blob_name)
            
            container_client = await self._get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob=blob_name)
            
            exists = await blob_client.exists()
            
            logger.debug(f"üîç Blob exists check: {blob_name} = {exists}")
            
            return exists
            
        except Exception as e:
            logger.debug(f"Error checking blob existence: {e}")
            return False

    @with_retry(max_attempts=3, delay=0.5, backoff=2.0)
    async def delete_blob(self, container_name: str, blob_name: str) -> bool:
        """Delete blob with retry"""
        blob_name = self._validate_blob_name(blob_name)
        
        async with self._lock:
            try:
                container_client = await self._get_container_client(container_name)
                blob_client = container_client.get_blob_client(blob=blob_name)
                
                await blob_client.delete_blob()
                
                logger.info(f"‚úÖ Deleted {blob_name} from {container_name}")
                
                await self._track_successful_operation("deletes")
                
                return True
                
            except ResourceNotFoundError:
                logger.debug(f"‚ö†Ô∏è Blob {blob_name} not found (already deleted)")
                return False
            except Exception as e:
                await self._handle_operation_error(e, "deletes")
                raise RuntimeError(f"Failed to delete {blob_name}: {e}")

    async def batch_download_blobs(self, container_name: str, blob_names: List[str], 
                                  max_concurrent: int = 20) -> Dict[str, bytes]:
        """Download multiple blobs in parallel with progress tracking"""
        if not blob_names:
            return {}
        
        # Validate all blob names
        validated_names = []
        for name in blob_names:
            try:
                validated_names.append(self._validate_blob_name(name))
            except ValueError as e:
                logger.warning(f"Skipping invalid blob name '{name}': {e}")
        
        if not validated_names:
            return {}
        
        # Optimize concurrency
        max_concurrent = min(max_concurrent, len(validated_names), 50)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        logger.info(f"üì• Batch downloading {len(validated_names)} blobs with {max_concurrent} workers")
        
        start_time = time.time()
        downloaded = {}
        failed = []
        
        async def download_with_progress(blob_name: str, index: int) -> Tuple[str, Optional[bytes]]:
            async with semaphore:
                try:
                    logger.debug(f"üì• [{index}/{len(validated_names)}] Downloading {blob_name}")
                    
                    data = await self.download_blob_as_bytes(container_name, blob_name)
                    
                    logger.debug(f"‚úÖ [{index}/{len(validated_names)}] Downloaded {blob_name} ({len(data)} bytes)")
                    
                    return (blob_name, data)
                    
                except Exception as e:
                    logger.error(f"‚ùå [{index}/{len(validated_names)}] Failed to download {blob_name}: {e}")
                    failed.append(blob_name)
                    return (blob_name, None)
        
        # Download all blobs in parallel
        tasks = [
            download_with_progress(name, i + 1) 
            for i, name in enumerate(validated_names)
        ]
        
        results = await asyncio.gather(*tasks)
        
        # Process results
        total_bytes = 0
        for blob_name, data in results:
            if data is not None:
                downloaded[blob_name] = data
                total_bytes += len(data)
        
        elapsed = time.time() - start_time
        speed_mbps = (total_bytes * 8 / (1024 * 1024)) / elapsed if elapsed > 0 else 0
        
        logger.info(f"‚úÖ Batch download complete: {len(downloaded)}/{len(validated_names)} successful")
        logger.info(f"   Total: {total_bytes / (1024 * 1024):.1f}MB in {elapsed:.1f}s @ {speed_mbps:.1f}Mbps")
        
        if failed:
            logger.warning(f"‚ö†Ô∏è Failed downloads: {failed[:5]}{'...' if len(failed) > 5 else ''}")
        
        return downloaded

    async def batch_upload_blobs(self, container_name: str, blobs: Dict[str, bytes], 
                                max_concurrent: int = 20) -> Dict[str, bool]:
        """Upload multiple blobs in parallel with progress tracking"""
        if not blobs:
            return {}
        
        # Validate all blob names and data
        validated_blobs = {}
        for name, data in blobs.items():
            try:
                validated_name = self._validate_blob_name(name)
                if isinstance(data, bytes):
                    validated_blobs[validated_name] = data
                else:
                    logger.warning(f"Skipping {name}: data must be bytes")
            except ValueError as e:
                logger.warning(f"Skipping invalid blob name '{name}': {e}")
        
        if not validated_blobs:
            return {}
        
        # Optimize concurrency
        max_concurrent = min(max_concurrent, len(validated_blobs), 50)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        logger.info(f"üì§ Batch uploading {len(validated_blobs)} blobs with {max_concurrent} workers")
        
        start_time = time.time()
        results = {}
        
        async def upload_with_progress(blob_name: str, data: bytes, index: int) -> Tuple[str, bool]:
            async with semaphore:
                try:
                    size_mb = len(data) / (1024 * 1024)
                    logger.debug(f"üì§ [{index}/{len(validated_blobs)}] Uploading {blob_name} ({size_mb:.1f}MB)")
                    
                    await self.upload_file(container_name, blob_name, data)
                    
                    logger.debug(f"‚úÖ [{index}/{len(validated_blobs)}] Uploaded {blob_name}")
                    
                    return (blob_name, True)
                    
                except Exception as e:
                    logger.error(f"‚ùå [{index}/{len(validated_blobs)}] Failed to upload {blob_name}: {e}")
                    return (blob_name, False)
        
        # Upload all blobs in parallel
        tasks = [
            upload_with_progress(name, data, i + 1) 
            for i, (name, data) in enumerate(validated_blobs.items())
        ]
        
        upload_results = await asyncio.gather(*tasks)
        
        # Process results
        successful = 0
        total_bytes = 0
        for blob_name, success in upload_results:
            results[blob_name] = success
            if success:
                successful += 1
                total_bytes += len(validated_blobs[blob_name])
        
        elapsed = time.time() - start_time
        speed_mbps = (total_bytes * 8 / (1024 * 1024)) / elapsed if elapsed > 0 else 0
        
        logger.info(f"‚úÖ Batch upload complete: {successful}/{len(validated_blobs)} successful")
        logger.info(f"   Total: {total_bytes / (1024 * 1024):.1f}MB in {elapsed:.1f}s @ {speed_mbps:.1f}Mbps")
        
        return results

    async def list_document_ids(self, container_name: str) -> List[str]:
        """List all document IDs by finding _context.txt files"""
        try:
            # Find all context files
            context_files = await self.list_blobs(
                container_name=container_name,
                suffix="_context.txt"
            )
            
            # Extract document IDs
            document_ids = []
            for filename in context_files:
                if filename.endswith("_context.txt"):
                    doc_id = filename[:-12]  # Remove "_context.txt"
                    document_ids.append(doc_id)
            
            # Remove duplicates and sort
            document_ids = sorted(list(set(document_ids)))
            
            logger.info(f"‚úÖ Found {len(document_ids)} documents in '{container_name}'")
            
            return document_ids
            
        except Exception as e:
            logger.error(f"Failed to list document IDs: {e}")
            raise RuntimeError(f"Document listing failed: {e}")

    async def delete_document_files(self, document_id: str) -> Dict[str, int]:
        """Delete all files associated with a document ID"""
        if not document_id or not re.match(r'^[\w\-]+$', document_id):
            raise ValueError("Invalid document ID")
        
        async with self._lock:
            try:
                deleted_counts = {
                    "pdfs": 0,
                    "pages": 0,
                    "metadata": 0,
                    "annotations": 0,
                    "chats": 0,
                    "sessions": 0,
                    "total": 0
                }
                
                # Delete from main container
                if hasattr(self.settings, 'AZURE_CONTAINER_NAME') and self.settings.AZURE_CONTAINER_NAME:
                    try:
                        await self.delete_blob(self.settings.AZURE_CONTAINER_NAME, f"{document_id}.pdf")
                        deleted_counts["pdfs"] = 1
                    except:
                        pass
                
                # Delete from cache container
                if hasattr(self.settings, 'AZURE_CACHE_CONTAINER_NAME') and self.settings.AZURE_CACHE_CONTAINER_NAME:
                    # List all files for this document
                    cache_files = await self.list_blobs(
                        container_name=self.settings.AZURE_CACHE_CONTAINER_NAME,
                        prefix=f"{document_id}_"
                    )
                    
                    # Categorize and count files
                    for filename in cache_files:
                        if "_page_" in filename and any(filename.endswith(ext) for ext in ['.png', '.jpg', '.jpeg']):
                            deleted_counts["pages"] += 1
                        elif filename.endswith(("_metadata.json", "_context.txt", "_document_index.json", "_grid_systems.json")):
                            deleted_counts["metadata"] += 1
                        elif "_annotations.json" in filename:
                            deleted_counts["annotations"] += 1
                        elif "_chat" in filename or "_all_chats.json" in filename:
                            deleted_counts["chats"] += 1
                        elif "_session_" in filename:
                            deleted_counts["sessions"] += 1
                    
                    # Batch delete for efficiency
                    if cache_files:
                        logger.info(f"üóëÔ∏è Deleting {len(cache_files)} files for document {document_id}")
                        
                        # Delete in batches
                        batch_size = 20
                        for i in range(0, len(cache_files), batch_size):
                            batch = cache_files[i:i+batch_size]
                            
                            delete_tasks = [
                                self.delete_blob(self.settings.AZURE_CACHE_CONTAINER_NAME, filename)
                                for filename in batch
                            ]
                            
                            results = await asyncio.gather(*delete_tasks, return_exceptions=True)
                            
                            for result in results:
                                if result is True:
                                    deleted_counts["total"] += 1
                                elif isinstance(result, Exception) and not isinstance(result, ResourceNotFoundError):
                                    logger.warning(f"Delete error: {result}")
                
                deleted_counts["total"] += deleted_counts["pdfs"]
                
                logger.info(f"‚úÖ Deleted {deleted_counts['total']} files for document '{document_id}'")
                logger.info(f"   Breakdown: PDFs={deleted_counts['pdfs']}, Pages={deleted_counts['pages']}, "
                           f"Metadata={deleted_counts['metadata']}, Annotations={deleted_counts['annotations']}, "
                           f"Chats={deleted_counts['chats']}, Sessions={deleted_counts['sessions']}")
                
                return deleted_counts
                
            except Exception as e:
                logger.error(f"Failed to delete document files: {e}")
                raise RuntimeError(f"Document deletion failed: {e}")

    def get_connection_info(self) -> Dict[str, Any]:
        """Get comprehensive information about the storage connection"""
        info = {
            "service_type": "Azure Blob Storage",
            "initialized": self._initialized,
            "connection_healthy": self._connection_healthy,
            "consecutive_failures": self._consecutive_failures,
            "last_successful_operation": datetime.fromtimestamp(self._last_successful_operation).isoformat() if self._last_successful_operation else None,
            "containers": {},
            "statistics": self._operation_stats,
            "configuration": {
                "timeouts": self.timeout_config,
                "retry": self.retry_config,
                "chunks": self.chunk_config
            },
            "optimizations": {
                "connection_pooling": "Container and blob clients cached",
                "concurrent_operations": "Up to 50 parallel operations",
                "retry_logic": f"{self.retry_config['max_retries']} retries with exponential backoff",
                "streaming": "Memory-efficient streaming for large files",
                "extended_timeouts": "Enabled for large file support"
            }
        }
        
        # Add configured containers
        if hasattr(self.settings, 'AZURE_CONTAINER_NAME') and self.settings.AZURE_CONTAINER_NAME:
            info["containers"]["main"] = self.settings.AZURE_CONTAINER_NAME
        if hasattr(self.settings, 'AZURE_CACHE_CONTAINER_NAME') and self.settings.AZURE_CACHE_CONTAINER_NAME:
            info["containers"]["cache"] = self.settings.AZURE_CACHE_CONTAINER_NAME
        
        # Add account info if available
        if self.settings.AZURE_STORAGE_CONNECTION_STRING:
            conn_str = self.settings.AZURE_STORAGE_CONNECTION_STRING
            if "AccountName=" in conn_str:
                account_match = re.search(r'AccountName=([^;]+)', conn_str)
                if account_match:
                    info["account_name"] = account_match.group(1)
        
        return info

    async def get_blob_properties(self, container_name: str, blob_name: str) -> Dict[str, Any]:
        """Get detailed blob properties"""
        try:
            blob_name = self._validate_blob_name(blob_name)
            
            container_client = await self._get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob=blob_name)
            
            properties = await blob_client.get_blob_properties()
            
            return {
                "name": properties.name,
                "size": properties.size,
                "size_mb": properties.size / (1024 * 1024),
                "content_type": properties.content_settings.content_type if properties.content_settings else None,
                "last_modified": properties.last_modified.isoformat() if properties.last_modified else None,
                "etag": properties.etag,
                "metadata": properties.metadata
            }
            
        except ResourceNotFoundError:
            raise FileNotFoundError(f"Blob '{blob_name}' not found")
        except Exception as e:
            logger.error(f"Failed to get blob properties: {e}")
            raise

    # === CONTEXT MANAGER ===

    async def __aenter__(self):
        """Async context manager entry"""
        await self.verify_connection()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit with cleanup"""
        try:
            # Clear client caches
            self._container_clients.clear()
            self._blob_clients.clear()
            
            # Close the main client if initialized
            if self._initialized and hasattr(self, 'blob_service_client'):
                await self.blob_service_client.close()
            
            logger.info("‚úÖ Storage service cleaned up successfully")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Storage service cleanup warning: {e}")

# === SINGLETON MANAGEMENT ===

_storage_instance: Optional[StorageService] = None
_storage_lock = asyncio.Lock()

async def get_storage_service() -> StorageService:
    """Get or create storage service instance (async)"""
    global _storage_instance
    
    async with _storage_lock:
        if _storage_instance is None:
            _storage_instance = StorageService()
            await _storage_instance.verify_connection()
    
    return _storage_instance

def get_storage_service_sync() -> StorageService:
    """Get or create storage service instance (sync) - for dependency injection"""
    global _storage_instance
    
    if _storage_instance is None:
        _storage_instance = StorageService()
    
    return _storage_instance

def reset_storage_service():
    """Reset the storage service (useful for testing)"""
    global _storage_instance
    _storage_instance = None
    logger.info("üîÑ Storage service reset")

# === EXPORT ===

__all__ = [
    'StorageService',
    'get_storage_service',
    'get_storage_service_sync',
    'reset_storage_service'
]